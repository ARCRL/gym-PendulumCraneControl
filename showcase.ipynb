{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [env_id]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "import pendulumCrane\n",
    "import gym\n",
    "from gym import wrappers, logger\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "class ddpgAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "    def __init__(self, env, action_space):\n",
    "        self.env = env\n",
    "        self.action_space = action_space\n",
    "        learning_rate = 0.001\n",
    "\n",
    "        self.load = 0\n",
    "        self.save = 0\n",
    "        self.training = 0\n",
    "        \n",
    "        self.state_size = 5\n",
    "        self.current_critic = critic(self.state_size + 1, 30, 1, learning_rate)\n",
    "        self.target_critic = critic(self.state_size + 1, 30, 1, learning_rate)\n",
    "        self.target_critic.load_state_dict(self.current_critic.state_dict())\n",
    "\n",
    "        self.current_actor = actor(self.state_size, 30, 1, learning_rate)\n",
    "        self.target_actor = actor(self.state_size, 30, 1, learning_rate)\n",
    "        self.target_actor.load_state_dict(self.current_actor.state_dict())\n",
    "\n",
    "        if self.load:\n",
    "            print(\"Loading saved state dicts\")\n",
    "            self.current_actor.load_state_dict(torch.load('./current_actor.pt'))\n",
    "            self.target_actor.load_state_dict(torch.load('./target_actor.pt'))\n",
    "            self.current_critic.load_state_dict(torch.load('./current_critic.pt'))\n",
    "            self.target_critic.load_state_dict(torch.load('./target_critic.pt'))\n",
    "\n",
    "\n",
    "        self.replay_memory_capacity = 100000\n",
    "\n",
    "        self.replay_memory = ReplayMemory(self.replay_memory_capacity)\n",
    "        print('state size: ', self.state_size)\n",
    "\n",
    "    def train(self):\n",
    "        epochs = 20\n",
    "        gamma = 0.99 # discount rate\n",
    "        tau = 0.01 # target network update rate\n",
    "        batch_size = 64\n",
    "        prefill_memory = True\n",
    "\n",
    "        count = 1\n",
    "\n",
    "        epoch_steps = 2000\n",
    "\n",
    "        noiseProcess_std = 0.1\n",
    "\n",
    "        loss_hist = np.zeros(1)\n",
    "        p_loss_hist = np.zeros(1)\n",
    "        reward = np.zeros(1)\n",
    "\n",
    "        if prefill_memory:\n",
    "            print('prefill replay memory')\n",
    "            s = env.reset()\n",
    "            while self.replay_memory.count() < self.replay_memory_capacity:\n",
    "                a = env.action_space.sample()\n",
    "                s1, r, d, _ = env.step(a)\n",
    "                s1 = s1[0]\n",
    "                self.replay_memory.add(s, a, r, s1, d)\n",
    "                s = s1\n",
    "                if d:\n",
    "                    s = env.reset()\n",
    "\n",
    "        batch = np.array(self.replay_memory.sample(2000))#,dtype=float)\n",
    "\n",
    "        # Extract from batch\n",
    "        ss, aa, rr, ss1, dd = np.stack(batch[:,0]), np.stack(batch[:,1]), np.stack(batch[:,2]), np.stack(batch[:,3]), np.stack(batch[:,4]).astype(int)\n",
    "\n",
    "        # Convert to Tensors\n",
    "        ss = (ss).reshape(-1, self.state_size)\n",
    "        aa = (aa).reshape(-1,1)\n",
    "        rr = (rr).reshape(-1,1)\n",
    "        ss1 = (ss1).reshape(-1,self.state_size)\n",
    "        dd = (dd).reshape(-1,1)\n",
    "\n",
    "        fig, ax = plt.subplots(5,1)\n",
    "        ax[0].plot(range(2000), ss)\n",
    "        ax[0].grid()\n",
    "        ax[1].plot(range(2000), aa)\n",
    "        ax[1].grid()\n",
    "        ax[2].plot(range(2000), rr)\n",
    "        ax[2].grid()\n",
    "        ax[3].plot(range(2000), ss1)\n",
    "        ax[3].grid()\n",
    "        ax[4].plot(range(2000), dd)\n",
    "        ax[4].grid()\n",
    "        #plt.show()\n",
    "\n",
    "        for e in range(epochs):\n",
    "            print(\"Starting epoch {}\".format(e))\n",
    "            s = env.reset()\n",
    "            env.env.mul = (e+1)*2#(e + 1)/2\n",
    "            for j in range(epoch_steps):\n",
    "                with torch.no_grad():\n",
    "                    a = self.current_actor(torch.from_numpy(s).float()).numpy() + np.random.normal(0, noiseProcess_std, 1)\n",
    "                a = np.clip(a,-10,10)\n",
    "                # Step with new action and save to memory\n",
    "                s1, r, d, _ = self.env.step(a[0])\n",
    "                s1 = s1[0]\n",
    "                self.replay_memory.add(s, a[0], r, s1, d)\n",
    "                \n",
    "                reward = np.append(reward,r)\n",
    "\n",
    "                # Update step\n",
    "                if self.replay_memory.count() >= batch_size:\n",
    "                    # sample batch from replay memory\n",
    "                    batch = np.array(self.replay_memory.sample(batch_size))#,dtype=float)\n",
    "\n",
    "                    # Extract from batch\n",
    "                    ss, aa, rr, ss1, dd = np.stack(batch[:,0]), np.stack(batch[:,1]), np.stack(batch[:,2]), np.stack(batch[:,3]), np.stack(batch[:,4]).astype(int)\n",
    "\n",
    "                    # Convert to Tensors\n",
    "                    ss = torch.from_numpy(ss).float().view(-1,self.state_size)\n",
    "                    aa = torch.from_numpy(aa).float().view(-1,1)\n",
    "                    rr = torch.from_numpy(rr).float().view(-1,1)\n",
    "                    ss1 = torch.from_numpy(ss1).float().view(-1,self.state_size)\n",
    "                    dd = torch.from_numpy(dd).float().view(-1,1)\n",
    "\n",
    "\n",
    "                    #with torch.no_grad():\n",
    "                    aa1 = self.target_actor(ss1)\n",
    "\n",
    "                    Qt_in = torch.cat((ss1, aa1),1)\n",
    "                    Qt = self.target_critic(Qt_in)\n",
    "\n",
    "\n",
    "                    self.current_critic.optimizer.zero_grad()\n",
    "\n",
    "                    y = rr + gamma * Qt * dd\n",
    "\n",
    "                    Qc_in = torch.cat([ss,aa],1)\n",
    "                    Qc = self.current_critic(Qc_in)\n",
    "\n",
    "                    ## Update critic\n",
    "                    loss = self.current_critic.loss(y,Qc)\n",
    "                    #loss1 = self.current_critic.MSEloss(Qc,y)/64\n",
    "\n",
    "                    loss.backward()\n",
    "\n",
    "                    self.current_critic.optimizer.step()\n",
    "                    self.target_critic.update_params(self.current_critic.state_dict(), tau)\n",
    "\n",
    "                    ## Update actor\n",
    "                    self.current_actor.optimizer.zero_grad()\n",
    "\n",
    "                    aa = self.current_actor(ss)\n",
    "\n",
    "                    Qc_in = torch.cat((ss, aa),1)\n",
    "                    Qc = -self.current_critic(Qc_in)\n",
    "\n",
    "                    Qc = torch.mean(Qc)\n",
    "                    Qc.backward()\n",
    "\n",
    "                    self.current_actor.optimizer.step()\n",
    "                    self.target_actor.update_params(self.current_actor.state_dict(), tau)\n",
    "\n",
    "                    loss_hist = np.append(loss_hist,loss.detach().numpy())\n",
    "                    p_loss_hist = np.append(p_loss_hist,Qc.detach().numpy())\n",
    "                \n",
    "                    count += 1\n",
    "                s = s1\n",
    "                if j%100 == 0: print(\"Epoch {}/{}: step {}/{}\\nReward = {}\\nAction = {}\\nDistance = {}\\nTarget = {}\\nX = {}\\nX = {}\\n\".format(e,epochs,j,epoch_steps,r,a,np.abs(s1[0]-s1[2]),s1[2],s[0], s[1]))\n",
    "                if d:\n",
    "                    print(\"Done reached\\nX = {}\\n\".format(s[0]))\n",
    "                    break\n",
    "            if self.save:\n",
    "                print(\"Saving models\")\n",
    "                torch.save(self.current_actor.state_dict(), './current_actor_new.pt')\n",
    "                torch.save(self.current_critic.state_dict(), './current_critic_new.pt')\n",
    "                torch.save(self.target_critic.state_dict(), './target_critic_new.pt')\n",
    "                torch.save(self.target_actor.state_dict(), './target_actor_new.pt')\n",
    "\n",
    "\n",
    "        fig, ax = plt.subplots(3,1)\n",
    "        ax[0].plot(range(count), loss_hist)\n",
    "        ax[0].set_xlabel(\"Steps\")\n",
    "        ax[0].set_ylabel(\"Value loss (critic)\")\n",
    "        ax[0].grid()\n",
    "\n",
    "        ax[1].plot(range(count), p_loss_hist)\n",
    "        ax[1].set_xlabel(\"Steps\")\n",
    "        ax[1].set_ylabel(\"Policy loss (actor)\")\n",
    "        ax[1].grid()\n",
    "\n",
    "        ax[2].plot(range(count), reward)\n",
    "        ax[2].set_xlabel(\"Steps\")\n",
    "        ax[2].set_ylabel(\"Rewards\")\n",
    "        ax[2].grid()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        batch = np.array(self.replay_memory.sample(2000))#,dtype=float)\n",
    "\n",
    "        # Extract from batch\n",
    "        ss, aa, rr, ss1, dd = np.stack(batch[:,0]), np.stack(batch[:,1]), np.stack(batch[:,2]), np.stack(batch[:,3]), np.stack(batch[:,4]).astype(int)\n",
    "\n",
    "        # Convert to Tensors\n",
    "        ss = (ss).reshape(-1,self.state_size)\n",
    "        aa = (aa).reshape(-1,1)\n",
    "        rr = (rr).reshape(-1,1)\n",
    "        ss1 = (ss1).reshape(-1,self.state_size)\n",
    "        dd = (dd).reshape(-1,1)\n",
    "\n",
    "        fig, ax = plt.subplots(5,1)\n",
    "        ax[0].plot(range(2000), ss)\n",
    "        ax[0].grid()\n",
    "        ax[1].plot(range(2000), aa)\n",
    "        ax[1].grid()\n",
    "        ax[2].plot(range(2000), rr)\n",
    "        ax[2].grid()\n",
    "        ax[3].plot(range(2000), ss1)\n",
    "        ax[3].grid()\n",
    "        ax[4].plot(range(2000), dd)\n",
    "        ax[4].grid()\n",
    "        #plt.show()\n",
    "\n",
    "        print(self.current_actor.hidden.weight)\n",
    "        #print(self.current_actor.hidden2.weight)\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        with torch.no_grad():\n",
    "            action = self.current_actor(torch.from_numpy(observation).float())\n",
    "        return np.clip(action.numpy()[0],-10,10)\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"Experience Replay Memory\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        #self.size = size\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, *args):\n",
    "        \"\"\"Add experience to memory.\"\"\"\n",
    "        self.memory.append([*args])\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample batch of experiences from memory with replacement.\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def count(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class actor(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs, learning_rate):\n",
    "        super(actor, self).__init__()\n",
    "        # network\n",
    "        self.hidden = nn.Linear(n_inputs, n_hidden)\n",
    "        self.hidden2 = nn.Linear(n_hidden, n_hidden)\n",
    "        #self.hidden3 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden, n_outputs)\n",
    "        # training\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=1e-8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = F.instance_norm(x)\n",
    "        x = self.hidden(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.instance_norm(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.hidden3(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = F.instance_norm(x)\n",
    "        x = self.out(x)\n",
    "        # x = F.tanh(x)\n",
    "        return torch.tanh(x)*10\n",
    "        #return x\n",
    "\n",
    "\n",
    "    def update_params(self, new_params, tau):\n",
    "        params = self.state_dict()\n",
    "        for k in params.keys():\n",
    "            params[k] = (1-tau) * params[k] + tau * new_params[k]\n",
    "        self.load_state_dict(params)\n",
    "\n",
    "class critic(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs, learning_rate):\n",
    "        super(critic, self).__init__()\n",
    "        # network\n",
    "        self.hidden = nn.Linear(n_inputs, n_hidden)\n",
    "        self.hidden2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden, n_outputs)\n",
    "        # training\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=1e-8)\n",
    "\n",
    "        self.MSEloss = nn.MSELoss(reduction='elementwise_mean')\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = F.instance_norm(x)\n",
    "        #print(x)\n",
    "        #print(self.hidden.weight)\n",
    "        x = self.hidden(x)\n",
    "        #print(x)\n",
    "        x = F.relu(x)\n",
    "        #print(x)\n",
    "        #x = F.batch_norm(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.batch_norm(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, q_outputs, q_targets):\n",
    "        return torch.mean(torch.pow(q_targets - q_outputs, 2))\n",
    "\n",
    "    def update_params(self, new_params, tau):\n",
    "        params = self.state_dict()\n",
    "        for k in params.keys():\n",
    "            params[k] = (1-tau) * params[k] + tau * new_params[k]\n",
    "        self.load_state_dict(params)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=None)\n",
    "    parser.add_argument('env_id', nargs='?', default='CartPoleCraneTrain-v2', help='Select the environment to run')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # You can set the level to logger.DEBUG or logger.WARN if you\n",
    "    # want to change the amount of output.\n",
    "    logger.set_level(logger.INFO)\n",
    "\n",
    "    env = gym.make(args.env_id)\n",
    "    env.env.set_goal(0)\n",
    "    \n",
    "    agent = ddpgAgent(env, env.action_space)\n",
    "    #agent.train()\n",
    "    # You provide the directory to write to (can be an existing\n",
    "    # directory, including one with existing data -- all monitor files\n",
    "    # will be namespaced). You can also dump to a tempdir if you'd\n",
    "    # like: tempfile.mkdtemp().\n",
    "\n",
    "    # You can set the level to logger.DEBUG or logger.WARN if you\n",
    "    # want to change the amount of output.\n",
    "    logger.set_level(logger.INFO)\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=None)\n",
    "    parser.add_argument('env_id', nargs='?', default='CartPoleCrane-v2', help='Select the environment to run')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "\n",
    "    env = gym.make(args.env_id)\n",
    "\n",
    "    outdir = '/tmp/ddpg-agent-results'\n",
    "    env = wrappers.Monitor(env, directory=outdir, force=True)\n",
    "\n",
    "    env.env.mul = 4\n",
    "    env.seed(1234)\n",
    "\n",
    "    episode_count = 10#2097865\n",
    "    reward = 0\n",
    "    done = False\n",
    "\n",
    "    for i in range(episode_count):\n",
    "        ob = env.reset()\n",
    "        #ob = env.env.reset('ddpg_' + str(i) +'.txt')\n",
    "        #env.env.env.state[2] = 0.05\n",
    "        #env.env.env.set_goal(0.8)\n",
    "        #while True:\n",
    "        #env.env.log()\n",
    "        print(ob[0], ob[-1])\n",
    "        for j in range(1500):\n",
    "            action = agent.act(ob, reward, done)\n",
    "            action = np.clip(action, -10, 10)\n",
    "            #action = -10.0\n",
    "            ob, reward, done, _ = env.step(action)\n",
    "            ob = ob[0]\n",
    "            #env.env.log()\n",
    "            if done:\n",
    "                break\n",
    "            # Note there's no env.render() here. But the environment still can open window and\n",
    "            # render if asked by env.monitor: it calls env.render('rgb_array') to record video.\n",
    "            # Video is not recorded every episode, see capped_cubic_video_schedule for details.\n",
    "\n",
    "    # Close the env and write monitor result info to disk\n",
    "    env.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Making new env: CartPoleCrane-v2\n",
      "StateSpaceDiscrete(\n",
      "array([[-1.89855434e-03, -5.31327561e-03,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 1.20559430e-01,  3.37396440e-01,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 6.16210292e-06,  1.74810578e-05,  1.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [-2.18335220e-04,  1.20991136e-03,  0.00000000e+00,\n",
      "         9.94166023e-01, -2.50653640e-01],\n",
      "       [-7.87649478e-06,  1.41947131e-05,  0.00000000e+00,\n",
      "         1.99499836e-02,  9.97491020e-01]]),\n",
      "array([[ 4.41690012e-01],\n",
      "       [ 1.96066911e+01],\n",
      "       [ 3.26992751e-04],\n",
      "       [-3.58022490e-02],\n",
      "       [-4.18525342e-04]]),\n",
      "array([[ 0.        ,  0.        ,  1.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        , 57.29577951]]),\n",
      "array([[0],\n",
      "       [0]]),\n",
      "dt: 0.02\n",
      ")\n",
      "INFO: Finished writing results. You can upload them to the scoreboard via gym.upload('/tmp/ddpgAgent')\n",
      "state size:  5\n",
      "INFO: Creating monitor directory tmp/vids\n",
      "INFO: Starting new video recorder writing to /home/casper/Dropbox/DL/gym-PendulumCraneControl/tmp/vids/openaigym.video.4.3901.video000000.mp4\n",
      "0.36844514354019675 0.09871755263540949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/casper/Dropbox/DL/gym/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Starting new video recorder writing to /home/casper/Dropbox/DL/gym-PendulumCraneControl/tmp/vids/openaigym.video.4.3901.video000001.mp4\n",
      "0.9801855178301693 0.45550704651237717\n",
      "0.6175776500756539 0.22964197048467905\n",
      "0.025880552925020006 0.22338444847607208\n",
      "0.15234517807899006 0.8626305566495539\n",
      "0.3945294121388194 0.5341606358134058\n",
      "0.8125190528981947 0.1418943075374638\n",
      "0.001256216819084166 0.40236135123020034\n",
      "INFO: Starting new video recorder writing to /home/casper/Dropbox/DL/gym-PendulumCraneControl/tmp/vids/openaigym.video.4.3901.video000008.mp4\n",
      "0.818767726068545 0.32729166371883345\n",
      "0.23809587885314643 0.7392472981420387\n"
     ]
    }
   ],
   "source": [
    "from pendulumCrane.agents.ddpg_agent import ddpgAgent\n",
    "import pendulumCrane\n",
    "import gym\n",
    "from gym import wrappers, logger\n",
    "import argparse\n",
    "from viewer import Viewer\n",
    "\n",
    "env = gym.make('CartPoleCrane-v2')\n",
    "agent = ddpgAgent(env, env.action_space)\n",
    "env = wrappers.Monitor(env, directory='tmp/vids', video_callable=None, force=True, write_upon_reset = False)\n",
    "env.seed(1234)\n",
    "\n",
    "episode_count = 10#2097865\n",
    "reward = 0\n",
    "done = False\n",
    "\n",
    "for i in range(episode_count):\n",
    "    ob = env.reset()\n",
    "    #view = Viewer(env, custom_render=True)\n",
    "    #ob = env.env.reset('ddpg_' + str(i) +'.txt')\n",
    "    #env.env.env.state[2] = 0.05\n",
    "    #env.env.env.set_goal(0.8)\n",
    "    #while True:\n",
    "    #env.env.log()\n",
    "    print(ob[0], ob[-1])\n",
    "    for j in range(1500):\n",
    "        #view.render()\n",
    "        action = agent.act(ob, reward, done)\n",
    "        action = np.clip(action, -10, 10)\n",
    "        #action = -10.0\n",
    "        ob, reward, done, _ = env.step(action)\n",
    "        ob = ob[0]\n",
    "        #env.env.log()\n",
    "        if done:\n",
    "            break\n",
    "        # Note there's no env.render() here. But the environment still can open window and\n",
    "        # render if asked by env.monitor: it calls env.render('rgb_array') to record video.\n",
    "        # Video is not recorded every episode, see capped_cubic_video_schedule for details.\n",
    "#view.render(close=True, display_gif=True)\n",
    "# Close the env and write monitor result info to disk\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
