{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showcase of the DDPG agent\n",
    "This notebook will showcase the code for the DDPG agent found in \"pendulumCrane/agent/ddpg_agent.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "import pendulumCrane\n",
    "import gym\n",
    "from gym import wrappers, logger\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor\n",
    "First the actor structure is defined in the actor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class actor(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs, learning_rate):\n",
    "        super(actor, self).__init__()\n",
    "        # network\n",
    "        self.hidden = nn.Linear(n_inputs, n_hidden)\n",
    "        self.hidden2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden, n_outputs)\n",
    "        # training\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=1e-8)\n",
    "\n",
    "    # Simple Forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return torch.tanh(x)*10\n",
    "\n",
    "\n",
    "    # For updating the parameters of the target network\n",
    "    def update_params(self, new_params, tau):\n",
    "        params = self.state_dict()\n",
    "        for k in params.keys():\n",
    "            params[k] = (1-tau) * params[k] + tau * new_params[k]\n",
    "        self.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic\n",
    "Next the critic network structure is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class critic(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs, learning_rate):\n",
    "        super(critic, self).__init__()\n",
    "        # network\n",
    "        self.hidden = nn.Linear(n_inputs, n_hidden)\n",
    "        self.hidden2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden, n_outputs)\n",
    "        # training\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=1e-8)\n",
    "\n",
    "        #self.MSEloss = nn.MSELoss(reduction='elementwise_mean')\n",
    "\n",
    "    # Simple Forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, q_outputs, q_targets):\n",
    "        return torch.mean(torch.pow(q_targets - q_outputs, 2))\n",
    "\n",
    "    # For updating the parameters of the target network\n",
    "    def update_params(self, new_params, tau):\n",
    "        params = self.state_dict()\n",
    "        for k in params.keys():\n",
    "            params[k] = (1-tau) * params[k] + tau * new_params[k]\n",
    "        self.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay memory\n",
    "The replay memory needed to save and sample transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"Experience Replay Memory\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        #self.size = size\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    # Add all in *args to the memory\n",
    "    def add(self, *args):\n",
    "        \"\"\"Add experience to memory.\"\"\"\n",
    "        self.memory.append([*args])\n",
    "    \n",
    "    # Get a batch\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample batch of experiences from memory with replacement.\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    # Get length (transitions) in the memory\n",
    "    def count(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DDPG agent class\n",
    "This class contains the function for training and using the DDPG agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ddpgAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "    def __init__(self, env, action_space):\n",
    "        self.env = env\n",
    "        self.action_space = action_space\n",
    "        learning_rate = 0.001\n",
    "\n",
    "        # If networks should be saved and or loaded and trained\n",
    "        self.load = 0\n",
    "        self.save = 0\n",
    "        self.training = 0\n",
    "        \n",
    "        # Initializes the actor/critic networks with random\n",
    "        self.state_size = 5\n",
    "        self.current_critic = critic(self.state_size + 1, 30, 1, learning_rate)\n",
    "        self.target_critic = critic(self.state_size + 1, 30, 1, learning_rate)\n",
    "        self.target_critic.load_state_dict(self.current_critic.state_dict())\n",
    "\n",
    "        self.current_actor = actor(self.state_size, 30, 1, learning_rate)\n",
    "        self.target_actor = actor(self.state_size, 30, 1, learning_rate)\n",
    "        self.target_actor.load_state_dict(self.current_actor.state_dict())\n",
    "\n",
    "        # Load the network if desired\n",
    "        if self.load:\n",
    "            print(\"Loading saved state dicts\")\n",
    "            self.current_actor.load_state_dict(torch.load('pendulumCrane/pretrainedNets/ddpg_current_actor.pt'))\n",
    "            self.target_actor.load_state_dict(torch.load('pendulumCrane/pretrainedNets/ddpg_target_actor.pt'))\n",
    "            self.current_critic.load_state_dict(torch.load('pendulumCrane/pretrainedNets/ddpg_current_critic.pt'))\n",
    "            self.target_critic.load_state_dict(torch.load('pendulumCrane/pretrainedNets/ddpg_target_critic.pt'))\n",
    "\n",
    "\n",
    "        self.replay_memory_capacity = 100000\n",
    "\n",
    "        self.replay_memory = ReplayMemory(self.replay_memory_capacity)\n",
    "        print('state size: ', self.state_size)\n",
    "    \n",
    "    def loadNet(self):\n",
    "        print(\"Loading saved state dicts\")\n",
    "        self.current_actor.load_state_dict(torch.load('pendulumCrane/pretrainedNets/ddpg_current_actor.pt'))\n",
    "        self.target_actor.load_state_dict(torch.load('pendulumCrane/pretrainedNets/ddpg_target_actor.pt'))\n",
    "        self.current_critic.load_state_dict(torch.load('pendulumCrane/pretrainedNets/ddpg_current_critic.pt'))\n",
    "        self.target_critic.load_state_dict(torch.load('pendulumCrane/pretrainedNets/ddpg_target_critic.pt'))\n",
    "\n",
    "    def train(self):\n",
    "        epochs = 20\n",
    "        gamma = 0.99 # discount rate\n",
    "        tau = 0.01 # target network update rate\n",
    "        batch_size = 64\n",
    "        prefill_memory = True\n",
    "\n",
    "        count = 1\n",
    "\n",
    "        epoch_steps = 2000\n",
    "\n",
    "        noiseProcess_std = 0.1\n",
    "\n",
    "        loss_hist = np.zeros(1)\n",
    "        p_loss_hist = np.zeros(1)\n",
    "        reward = np.zeros(1)\n",
    "\n",
    "        # Prefill memory\n",
    "        if prefill_memory:\n",
    "            print('prefill replay memory')\n",
    "            s = env.reset()\n",
    "            while self.replay_memory.count() < self.replay_memory_capacity:\n",
    "                a = env.action_space.sample()\n",
    "                s1, r, d, _ = env.step(a)\n",
    "                s1 = s1[0]\n",
    "                self.replay_memory.add(s, a, r, s1, d)\n",
    "                s = s1\n",
    "                if d:\n",
    "                    s = env.reset()\n",
    "\n",
    "        # Start the training loop\n",
    "        for e in range(epochs):\n",
    "            print(\"Starting epoch {}\".format(e + 1))\n",
    "            s = env.reset()\n",
    "            env.env.mul = (e+1)*2 # Update multiplyer for bell curve\n",
    "            for j in range(epoch_steps):\n",
    "                with torch.no_grad():\n",
    "                    a = self.current_actor(torch.from_numpy(s).float()).numpy() + np.random.normal(0, noiseProcess_std, 1)\n",
    "                a = np.clip(a,-10,10)\n",
    "                # Step with new action and save to memory\n",
    "                s1, r, d, _ = self.env.step(a[0])\n",
    "                s1 = s1[0]\n",
    "                self.replay_memory.add(s, a[0], r, s1, d)\n",
    "                \n",
    "                reward = np.append(reward,r)\n",
    "\n",
    "                # Update step Just following the ddpg algorithm from Lilicrap 2015\n",
    "                if self.replay_memory.count() >= batch_size:\n",
    "                    # sample batch from replay memory\n",
    "                    batch = np.array(self.replay_memory.sample(batch_size))#,dtype=float)\n",
    "\n",
    "                    # Extract from batch\n",
    "                    ss, aa, rr, ss1, dd = np.stack(batch[:,0]), np.stack(batch[:,1]), np.stack(batch[:,2]), np.stack(batch[:,3]), np.stack(batch[:,4]).astype(int)\n",
    "\n",
    "                    # Convert to Tensors\n",
    "                    ss = torch.from_numpy(ss).float().view(-1,self.state_size)\n",
    "                    aa = torch.from_numpy(aa).float().view(-1,1)\n",
    "                    rr = torch.from_numpy(rr).float().view(-1,1)\n",
    "                    ss1 = torch.from_numpy(ss1).float().view(-1,self.state_size)\n",
    "                    dd = torch.from_numpy(dd).float().view(-1,1)\n",
    "\n",
    "\n",
    "                    #with torch.no_grad():\n",
    "                    aa1 = self.target_actor(ss1)\n",
    "\n",
    "                    Qt_in = torch.cat((ss1, aa1),1)\n",
    "                    Qt = self.target_critic(Qt_in)\n",
    "\n",
    "\n",
    "                    self.current_critic.optimizer.zero_grad()\n",
    "\n",
    "                    y = rr + gamma * Qt * dd\n",
    "\n",
    "                    Qc_in = torch.cat([ss,aa],1)\n",
    "                    Qc = self.current_critic(Qc_in)\n",
    "\n",
    "                    ## Update critic\n",
    "                    loss = self.current_critic.loss(y,Qc)\n",
    "                    #loss1 = self.current_critic.MSEloss(Qc,y)/64\n",
    "\n",
    "                    loss.backward()\n",
    "\n",
    "                    self.current_critic.optimizer.step()\n",
    "                    self.target_critic.update_params(self.current_critic.state_dict(), tau)\n",
    "\n",
    "                    ## Update actor\n",
    "                    self.current_actor.optimizer.zero_grad()\n",
    "\n",
    "                    aa = self.current_actor(ss)\n",
    "\n",
    "                    Qc_in = torch.cat((ss, aa),1)\n",
    "                    Qc = -self.current_critic(Qc_in)\n",
    "\n",
    "                    Qc = torch.mean(Qc)\n",
    "                    Qc.backward()\n",
    "\n",
    "                    self.current_actor.optimizer.step()\n",
    "                    self.target_actor.update_params(self.current_actor.state_dict(), tau)\n",
    "\n",
    "                    loss_hist = np.append(loss_hist,loss.detach().numpy())\n",
    "                    p_loss_hist = np.append(p_loss_hist,Qc.detach().numpy())\n",
    "                \n",
    "                    count += 1\n",
    "                s = s1\n",
    "                if j%100 == 0: print(\"Epoch {}/{}: step {}/{}\\nReward = {}\\nAction = {}\\nDistance = {}\\nTarget = {}\\nX = {}\\nX = {}\\n\".format(e+1,epochs+1,j,epoch_steps,r,a,np.abs(s1[0]-s1[2]),s1[2],s[0], s[1]))\n",
    "                if d:\n",
    "                    print(\"Done reached\\nX = {}\\n\".format(s[0]))\n",
    "                    break\n",
    "            # Saves the networks\n",
    "            if self.save:\n",
    "                print(\"Saving models\")\n",
    "                torch.save(self.current_actor.state_dict(), 'pendulumCrane/pretrainedNets/ddpg_current_actor.pt')\n",
    "                torch.save(self.current_critic.state_dict(), 'pendulumCrane/pretrainedNets/ddpg_current_critic.pt')\n",
    "                torch.save(self.target_critic.state_dict(), 'pendulumCrane/pretrainedNets/ddpg_target_critic.pt')\n",
    "                torch.save(self.target_actor.state_dict(), 'pendulumCrane/pretrainedNets/ddpg_target_actor.pt')\n",
    "\n",
    "        # Plots losses an rewards\n",
    "        fig, ax = plt.subplots(3,1)\n",
    "        ax[0].plot(range(count), loss_hist)\n",
    "        ax[0].set_xlabel(\"Steps\")\n",
    "        ax[0].set_ylabel(\"Value loss (critic)\")\n",
    "        ax[0].grid()\n",
    "\n",
    "        ax[1].plot(range(count), p_loss_hist)\n",
    "        ax[1].set_xlabel(\"Steps\")\n",
    "        ax[1].set_ylabel(\"Policy loss (actor)\")\n",
    "        ax[1].grid()\n",
    "\n",
    "        ax[2].plot(range(count), reward)\n",
    "        ax[2].set_xlabel(\"Steps\")\n",
    "        ax[2].set_ylabel(\"Rewards\")\n",
    "        ax[2].grid()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # Do an action based on an observed state\n",
    "    def act(self, observation, reward, done):\n",
    "        with torch.no_grad():\n",
    "            action = self.current_actor(torch.from_numpy(observation).float())\n",
    "        return np.clip(action.numpy()[0],-10,10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main script\n",
    "Calling the appropriate functions and networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateSpaceDiscrete(\n",
      "array([[-1.89855434e-03, -5.31327561e-03,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 1.20559430e-01,  3.37396440e-01,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 6.16210292e-06,  1.74810578e-05,  1.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [-2.18335220e-04,  1.20991136e-03,  0.00000000e+00,\n",
      "         9.94166023e-01, -2.50653640e-01],\n",
      "       [-7.87649478e-06,  1.41947131e-05,  0.00000000e+00,\n",
      "         1.99499836e-02,  9.97491020e-01]]),\n",
      "array([[ 4.41690012e-01],\n",
      "       [ 1.96066911e+01],\n",
      "       [ 3.26992751e-04],\n",
      "       [-3.58022490e-02],\n",
      "       [-4.18525342e-04]]),\n",
      "array([[ 0.        ,  0.        ,  1.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        , 57.29577951]]),\n",
      "array([[0],\n",
      "       [0]]),\n",
      "dt: 0.02\n",
      ")\n",
      "state size:  5\n",
      "Loading saved state dicts\n",
      "StateSpaceDiscrete(\n",
      "array([[-1.89855434e-03, -5.31327561e-03,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 1.20559430e-01,  3.37396440e-01,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 6.16210292e-06,  1.74810578e-05,  1.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [-2.18335220e-04,  1.20991136e-03,  0.00000000e+00,\n",
      "         9.94166023e-01, -2.50653640e-01],\n",
      "       [-7.87649478e-06,  1.41947131e-05,  0.00000000e+00,\n",
      "         1.99499836e-02,  9.97491020e-01]]),\n",
      "array([[ 4.41690012e-01],\n",
      "       [ 1.96066911e+01],\n",
      "       [ 3.26992751e-04],\n",
      "       [-3.58022490e-02],\n",
      "       [-4.18525342e-04]]),\n",
      "array([[ 0.        ,  0.        ,  1.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        , 57.29577951]]),\n",
      "array([[0],\n",
      "       [0]]),\n",
      "dt: 0.02\n",
      ")\n",
      "0.36844514354019675 0.09871755263540949\n",
      "0.9801855178301693 0.45550704651237717\n",
      "0.6175776500756539 0.22964197048467905\n",
      "0.025880552925020006 0.22338444847607208\n",
      "0.15234517807899006 0.8626305566495539\n",
      "0.3945294121388194 0.5341606358134058\n",
      "0.8125190528981947 0.1418943075374638\n",
      "0.001256216819084166 0.40236135123020034\n",
      "0.818767726068545 0.32729166371883345\n",
      "0.23809587885314643 0.7392472981420387\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPoleCraneTrain-v2')\n",
    "agent = ddpgAgent(env, env.action_space)\n",
    "    \n",
    "agent.save = 0\n",
    "agent.training = 0\n",
    "\n",
    "# Uncomment to load a pretrained agent\n",
    "#agent.loadNet()\n",
    "\n",
    "if agent.training:\n",
    "    agent.train()\n",
    "\n",
    "env = gym.make('CartPoleCrane-v2')\n",
    "\n",
    "outdir = 'tmp/ddpg-agent-results'\n",
    "env = wrappers.Monitor(env, directory=outdir, force=True)\n",
    "\n",
    "env.env.mul = 4\n",
    "env.seed(1234)\n",
    "\n",
    "episode_count = 10#2097865\n",
    "reward = 0\n",
    "done = False\n",
    "\n",
    "for i in range(episode_count):\n",
    "    ob = env.reset()\n",
    "\n",
    "    print(ob[0], ob[-1])\n",
    "    for j in range(1500):\n",
    "        action = agent.act(ob, reward, done)\n",
    "        action = np.clip(action, -10, 10)\n",
    "\n",
    "        ob, reward, done, _ = env.step(action)\n",
    "        ob = ob[0]\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        # Note there's no env.render() here. But the environment still can open window and\n",
    "        # render if asked by env.monitor: it calls env.render('rgb_array') to record video.\n",
    "        # Video is not recorded every episode, see capped_cubic_video_schedule for details.\n",
    "\n",
    "# Close the env and write monitor result info to disk\n",
    "env.env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
