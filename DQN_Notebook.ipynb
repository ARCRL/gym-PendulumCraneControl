{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showcase of the DQN agent\n",
    "This notebook will showcase the code for the DQN agent found in \"pendulumCrane/agent/dqn_agent.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import gym\n",
    "import pendulumCrane\n",
    "\n",
    "from gym import wrappers, logger\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "We use a replay buffer to store transitions and sample from this buffer when training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replay buffer\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"Experience Replay Memory\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        #self.size = size\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, *args):\n",
    "        \"\"\"Add experience to memory.\"\"\"\n",
    "        self.memory.append([*args])\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample batch of experiences from memory with replacement.\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def count(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Network\n",
    "The network is a standard feed forward neural net, with 3 hidden layers and 128 hidden units in each, with dropout between each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN class\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs, learning_rate):\n",
    "        super(DQN, self).__init__()\n",
    "        self.hidden_size = 128\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(n_inputs,self.hidden_size)\n",
    "        self.fc2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.fc3 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.fc4 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.fc5 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        self.out = nn.Linear(self.hidden_size, n_outputs)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "        # define dropout\n",
    "        self.dropout = torch.nn.Dropout(p=0.5, inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Standard FNN layers\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        return F.softmax(x, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions and reward calculation\n",
    "To use a continuous environment, we pass the neuron index of the selected action through a small pseudo DAC, which converts it to a voltage in the D2C function.\n",
    "\n",
    "We also calculate the reward used internally in the agent, as the environment was shared between DQN and DPPG, which uses different reward structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the index of the output neuron, to a continuous value for the environment\n",
    "def D2C(discrete_action):\n",
    "    pos_voltage = 2.0 # Go left\n",
    "    neg_voltage = -2.0 # Go right\n",
    "    volt_range = np.array([-9.0, -7.0, -5.0, -2.0, -1.0, -0.1, 0.0, 0.1, 1.0, 2.0, 5.0, 7.0, 9.0])\n",
    "    output = volt_range[int(discrete_action)]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def agent_reward(action, next_state, reward_array, old_state):\n",
    "\n",
    "    # Calculate distance \n",
    "    x_goal = next_state[2]\n",
    "    x_pos = next_state[0]\n",
    "    distance = x_pos - x_goal\n",
    "\n",
    "    x_goal_old = old_state[2]\n",
    "    x_pos_old = old_state[0]\n",
    "    old_distance = x_pos_old - x_goal_old\n",
    "\n",
    "    # Calculate end effector position of pendulum\n",
    "    theta = next_state[1]\n",
    "\n",
    "    pen_length = 0.7 # Length of pendulum - Placeholder\n",
    "    theta_pos = x_pos + math.sin(theta)*pen_length\n",
    "\n",
    "    # Use distance of end effector instead of sledge\n",
    "    distance_pendulum = theta_pos - x_goal\n",
    "\n",
    "    dist_dif = distance**2 - old_distance**2\n",
    "\n",
    "    reward = 0\n",
    "    reward_type = 0\n",
    "    # Base reward:\n",
    "    if dist_dif > 0:\n",
    "        reward = -1\n",
    "        reward_type = 1\n",
    "    elif dist_dif == 0:\n",
    "        reward = 1\n",
    "        reward_type = 0\n",
    "    elif dist_dif < 0:\n",
    "        reward = 1\n",
    "        reward_type = 2\n",
    "\n",
    "\n",
    "    reward_array[reward_type] += 1 \n",
    "\n",
    "\n",
    "    # Distance bonus\n",
    "    dist_bonus = 2 - abs(distance)*10\n",
    "    reward = reward + dist_bonus\n",
    "\n",
    "    # Theta_pos penalty\n",
    "    theta_p = next_state[1]\n",
    "    theta_p = (x_pos+math.sin(theta_p)*0.7)-x_pos\n",
    "    theta_penalty = abs(theta_p)*2\n",
    "\n",
    "    reward = reward - theta_penalty\n",
    "    return reward, reward_array\n",
    "\n",
    "def random_action(n_outputs):\n",
    "    action = np.random.randint(n_outputs,size=1)\n",
    "    action = float(action)\n",
    "    return action\n",
    "\n",
    "\n",
    "\n",
    "def select_action(state, step_count):\n",
    "    # Determine if we are taking a random action or not\n",
    "    Eps_start = 0.9\n",
    "    Eps_end = 0.15\n",
    "    Eps_decay = 25\n",
    "\n",
    "    sample = random.random()\n",
    "    Eps_threshold = Eps_end + (Eps_start - Eps_end) * math.exp(-1. *step_count/Eps_decay)\n",
    "    #step_count += 1\n",
    "\n",
    "    action_type = \"R\" # Random action\n",
    "\n",
    "    if sample > Eps_threshold: \n",
    "        action_type = \"Q\" # Q action\n",
    "\n",
    "    return action_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "The optimizer samples random transitions from the replay buffer and uses them as a basis for training the network, based on the Huber Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_optimizer(n_inputs):\n",
    "    if replay_memory.count() < batch_size:\n",
    "        return\n",
    "\n",
    "    global num_param_updates\n",
    "    # sample batch from replay memory\n",
    "    batch = np.array(replay_memory.sample(batch_size))#,dtype=float)\n",
    "\n",
    "\n",
    "    # Extract from batch\n",
    "    ss, aa, rr, ss1, dd = np.stack(batch[:,0]), np.stack(batch[:,1]), np.stack(batch[:,2]), np.stack(batch[:,3]), np.stack(batch[:,4]).astype(int)\n",
    "\n",
    "    # Convert to Tensors\n",
    "    ss = torch.from_numpy(ss).float().view(-1,n_inputs)\n",
    "    aa = torch.from_numpy(aa).long().view(-1,1)\n",
    "    rr = torch.from_numpy(rr).float().view(-1,1)\n",
    "    ss1 = torch.from_numpy(ss1).float().view(-1,n_inputs)\n",
    "    dd = torch.from_numpy(dd).float().view(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "    # Forward pass on batch\n",
    "    policy_net.optimizer.zero_grad()\n",
    "    Q = policy_net(ss.float())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get the q value for all possible moves\n",
    "        Q1 = policy_net(ss1.float())\n",
    "\n",
    "    yk = Q1.clone()\n",
    "    for k in range(batch_size):\n",
    "        yk_k = rr[k] + gamma * Q1[k].max().item() * (not dd[k])\n",
    "        yk[k, aa[k]] = yk_k\n",
    "\n",
    "    ## update network weights\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(Q, yk)\n",
    "\n",
    "    # Old loss function\n",
    "    #loss = policy_net.loss(Q, yk)\n",
    "\n",
    "    loss.backward()\n",
    "    policy_net.optimizer.step()\n",
    "\n",
    "    num_param_updates += 1\n",
    "\n",
    "    if num_param_updates % target_update_freq == 0:\n",
    "        # update target network parameters from policy network parameters\n",
    "        target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary training loop\n",
    "Specify the OpenAI Gym environment, and define hyper parameters.\n",
    "Select if the system should load a previously saved network, if the new network should be saved after training, and if the system should train or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/casper/Dropbox/DL/gym/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateSpaceDiscrete(\n",
      "array([[-1.89855434e-03, -5.31327561e-03,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 1.20559430e-01,  3.37396440e-01,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 6.16210292e-06,  1.74810578e-05,  1.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [-2.18335220e-04,  1.20991136e-03,  0.00000000e+00,\n",
      "         9.94166023e-01, -2.50653640e-01],\n",
      "       [-7.87649478e-06,  1.41947131e-05,  0.00000000e+00,\n",
      "         1.99499836e-02,  9.97491020e-01]]),\n",
      "array([[ 4.41690012e-01],\n",
      "       [ 1.96066911e+01],\n",
      "       [ 3.26992751e-04],\n",
      "       [-3.58022490e-02],\n",
      "       [-4.18525342e-04]]),\n",
      "array([[ 0.        ,  0.        ,  1.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        , 57.29577951]]),\n",
      "array([[0],\n",
      "       [0]]),\n",
      "dt: 0.02\n",
      ")\n",
      "Number of inputs:  3 , number of outputs:  13\n",
      "Tau:  0.02  Number of steps:  800  Episode duration:  16.0  [s]\n",
      "Fixed states:  [[0.05 0.   0.95]\n",
      " [0.95 0.   0.05]\n",
      " [0.25 0.   0.75]\n",
      " [0.75 0.   0.25]]\n",
      "Start training\n",
      "  10. mean training reward: -292.84, mean validation reward: 152.81\n",
      "  20. mean training reward: 228.70, mean validation reward: -1421.82\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    " ####### MAIN #######\n",
    "\n",
    "# Import environment\n",
    "env = gym.make('CartPoleCrane-v3')\n",
    "\n",
    "# Number of inputs and outputs\t\n",
    "n_inputs = 3 \n",
    "n_outputs = 13 \n",
    "print(\"Number of inputs: \", n_inputs, \", number of outputs: \", n_outputs)\n",
    "\n",
    "##### train Deep Q-network #####\n",
    "\n",
    "# Parameters\n",
    "num_episodes = 20 # default: 50 # Number of episodes\n",
    "episode_limit = 800 # Length of episode\n",
    "\n",
    "batch_size = 32 # Default: 32\n",
    "learning_rate = 0.002 # Default = 0.002\n",
    "gamma = 0.99 # discount rate\n",
    "replay_memory_capacity = 5000\n",
    "prefill_memory = False\n",
    "val_freq = 10 # validation frequency\n",
    "\n",
    "\n",
    "num_param_updates = 0\n",
    "target_update_freq = 25 # Default 25\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cpu\")\n",
    "dtype = torch.FloatTensor\n",
    "dlongtype = torch.LongTensor\n",
    "\n",
    "# initialize DQN\n",
    "policy_net = DQN(n_inputs, n_outputs, learning_rate).to(device)\n",
    "target_net = DQN(n_inputs, n_outputs, learning_rate).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "\n",
    "## Load network \n",
    "load = 0\n",
    "save = 0\n",
    "train = 1\n",
    "if load:\n",
    "\tprint(\"Loading network\")\n",
    "\tpolicy_net.load_state_dict(torch.load('pendulumCrane/pretrainedNets/dqn_net.pt'))\n",
    "\ttarget_net.load_state_dict(torch.load('pendulumCrane/pretrainedNets/dqn_net.pt'))\n",
    "\n",
    "if not train:\n",
    "\tprint(\"Skipping Training\")\n",
    "\tdata_array = np.array([0,0,0,0,0,0])\n",
    "\tnum_episodes = 0\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = ReplayMemory(replay_memory_capacity)\n",
    "\n",
    "#eps_check(num_episodes)\n",
    "\n",
    "val_ep = False\n",
    "\n",
    "\n",
    "# Tau from environment\n",
    "tau_agent = env.env.tau\n",
    "print(\"Tau: \", tau_agent, \" Number of steps: \", episode_limit, \" Episode duration: \", tau_agent*episode_limit, \" [s]\")\n",
    "\n",
    "# Collect a set of fixed states for average Q metric\n",
    "state = env.reset()\n",
    "\n",
    "s_fix_1 = state # x, theta, goal\n",
    "s_fix_1[0] = 0.05\n",
    "s_fix_1[2] = 0.95\n",
    "\n",
    "state = env.reset()\n",
    "s_fix_2 = state\n",
    "s_fix_2[0] = 0.95\n",
    "s_fix_2[2] = 0.05\n",
    "\n",
    "state = env.reset()\n",
    "s_fix_3 = state\n",
    "s_fix_3[0] = 0.25\n",
    "s_fix_3[2] = 0.75\n",
    "\n",
    "state = env.reset()\n",
    "s_fix_4 = state\n",
    "s_fix_4[0] = 0.75\n",
    "s_fix_4[2] = 0.25\n",
    "\n",
    "fixed_states = np.array([s_fix_1, s_fix_2, s_fix_3, s_fix_4])\n",
    "print(\"Fixed states: \", fixed_states)\n",
    "\n",
    "Q_average = []\n",
    "\n",
    "output_histogram = np.zeros(n_outputs)\n",
    "reward_array = np.zeros(4)\n",
    "\n",
    "\n",
    "## Training loop\n",
    "print(\"Start training\")\n",
    "rewards, lengths = [], []\n",
    "final_distances = []\n",
    "initial_distances = []\n",
    "state = env.reset()\n",
    "\n",
    "## Data\n",
    "#data_array = np.array([0,0,0,0])\n",
    "#print(\"Size of array :\", data_array.size)\n",
    "\n",
    "\n",
    "epsilon = 1.0\n",
    "\n",
    "step_count = 0\n",
    "max_theta = 0\n",
    "for i in range(num_episodes):\n",
    "\n",
    "\t# Reset environment and values\n",
    "\t# Get initial observation \n",
    "\tstate = env.reset()\n",
    "\tep_reward = 0\n",
    "\tfinal_distance = 0\n",
    "\n",
    "\t# Initialize Action array storage\n",
    "\taction_array = np.array([[0,0,0,0]])\n",
    "\n",
    "\t# Initial distance\n",
    "\tx_goal = state[2]\n",
    "\tx_pos = state[0]\n",
    "\tinitial_distance = x_pos - x_goal\n",
    "\tdistance = initial_distance\n",
    "\n",
    "\tfor j in range(episode_limit):\n",
    "\n",
    "\t\t# Select action based on states, the epsilon greedy strategy and validation episode flag\n",
    "\t\taction_type = select_action(state, step_count)\n",
    "\n",
    "\t\t#if np.random.rand() >= epsilon:\n",
    "\t\tif action_type == \"Q\":\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\t# Get the q value for all possible moves\n",
    "\t\t\t\tq_value_all_actions = policy_net(torch.from_numpy(state).float())\n",
    "\t\t\t\t# Select the action that has the highest value\n",
    "\t\t\t\taction = q_value_all_actions.argmax().item()\n",
    "\n",
    "\t\t\t\t# Update output histogram\n",
    "\t\t\t\toutput_histogram[int(action)] += 1\n",
    "\n",
    "\t\telse:\n",
    "\t\t\taction = random_action(n_outputs)\n",
    "\n",
    "\t\t# Perform action and get results\n",
    "\t\tnumpy_action = D2C(action) # Discrete action as a numpy variable\n",
    "\t\tnext_state, reward_old, done, _ = env.step(numpy_action)\n",
    "\n",
    "\t\t# Old distance\n",
    "\t\told_distance = distance\n",
    "\n",
    "\t\t# New distance \n",
    "\t\tx_goal = next_state[2]\n",
    "\t\tx_pos = next_state[0]\n",
    "\t\tdistance = x_pos - x_goal\n",
    "\n",
    "\n",
    "\t\t# get reward\n",
    "\t\treward, reward_array = agent_reward(numpy_action, next_state, reward_array, state)\n",
    "\n",
    "\t\t# Check if done\n",
    "\t\tif done:\n",
    "\t\t\tlengths.append(j + 1)\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\treplay_memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "\t\t# Perform model optimization (It automatically checks if replay buffer is full)\n",
    "\t\tif (j+1) % 2 == 0:\n",
    "\t\t\tsimple_optimizer(n_inputs)\n",
    "\n",
    "\n",
    "\t\t# bookkeeping\n",
    "\t\tstate = next_state\n",
    "\t\tep_reward += reward\n",
    "\n",
    "\t### End of episode\n",
    "\t\n",
    "\tfinal_distances.append(distance)\n",
    "\trewards.append(ep_reward)\n",
    "\tlengths.append(j+1)\n",
    "\tinitial_distances.append(initial_distance)\n",
    "\n",
    "\n",
    "\n",
    "\tstep_count += 1\n",
    "\t# Validation episode\n",
    "\tif (i+1) % val_freq == 0:\n",
    "\t\tvalidation_rewards = []\n",
    "\t\t#q_diff = []\n",
    "\t\tfor ii in range(4):\n",
    "\t\t\tdata_array = np.array([0,0,0,0,0,0])\n",
    "\t\t\ts = env.reset()\n",
    "\n",
    "\t\t\t# Fixed state for validation\n",
    "\t\t\ts[0] = 0.05 # Start pos\n",
    "\t\t\ts[2] = 0.80 # Goal pos\n",
    "\t\t\tenv.env.state[2] = s[0]\n",
    "\t\t\tenv.env.set_goal(s[2])\n",
    "\n",
    "\t\t\treward_val = 0\n",
    "\t\t\tfor jj in range(episode_limit):\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\t# Get the q value for all possible moves\n",
    "\t\t\t\t\tQ_probs = policy_net(torch.from_numpy(s).float())\n",
    "\t\t\t\t\t# Select the action that has the highest value\n",
    "\t\t\t\t\taction = Q_probs.argmax().item()\n",
    "\n",
    "\t\t\t\ta_env = D2C(action)\n",
    "\t\t\t\ts_new, r_old, done, _ = env.step(a_env)\n",
    "\n",
    "\t\t\t\told_distance = distance\n",
    "\t\t\t\t# Update distance\n",
    "\t\t\t\tx_goal = s[2]\n",
    "\t\t\t\tx_pos = s[0]\n",
    "\t\t\t\tdistance = x_pos - x_goal\n",
    "\n",
    "\t\t\t\t# Calculate actual reward\n",
    "\t\t\t\tr, reward_array = agent_reward(a_env, s_new, reward_array, s)\n",
    "\t\t\t\t#q_diff = qd\n",
    "\n",
    "\t\t\t\ttime_stamp = tau_agent*jj\n",
    "\t\t\t\t## Data \n",
    "\t\t\t\treward_val += r\n",
    "\t\t\t\ts = s_new\n",
    "\t\t\t\tif done: \n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t#print(\"Breaking, ii: \", ii, \" jj: \", jj)\n",
    "\t\t\tvalidation_rewards.append(reward_val)\n",
    "\t\tprint('{:4d}. mean training reward: {:6.2f}, mean validation reward: {:6.2f}'.format(i+1, np.mean(rewards[-val_freq:]), np.mean(validation_rewards)))\n",
    "\n",
    "\t\t# Average Q value on fixed states\n",
    "\t\tQ_ep = []\n",
    "\t\tfor ii in range(4):\n",
    "\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\t# Get the q value for all possible moves\n",
    "\t\t\t\tQ_probs = policy_net(torch.from_numpy(fixed_states[ii]).float())\n",
    "\t\t\t# get the highest value action\n",
    "\t\t\tmax_action = Q_probs.max().item()\n",
    "\t\t\tQ_ep.append(max_action)\n",
    "\t\tQ_average.append(np.mean(Q_ep))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After training\n",
    "Functions to save the network, plot general data and display a simulation of the resulting network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateSpaceDiscrete(\n",
      "array([[-1.89855434e-03, -5.31327561e-03,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 1.20559430e-01,  3.37396440e-01,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 6.16210292e-06,  1.74810578e-05,  1.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [-2.18335220e-04,  1.20991136e-03,  0.00000000e+00,\n",
      "         9.94166023e-01, -2.50653640e-01],\n",
      "       [-7.87649478e-06,  1.41947131e-05,  0.00000000e+00,\n",
      "         1.99499836e-02,  9.97491020e-01]]),\n",
      "array([[ 4.41690012e-01],\n",
      "       [ 1.96066911e+01],\n",
      "       [ 3.26992751e-04],\n",
      "       [-3.58022490e-02],\n",
      "       [-4.18525342e-04]]),\n",
      "array([[ 0.        ,  0.        ,  1.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        , 57.29577951]]),\n",
      "array([[0],\n",
      "       [0]]),\n",
      "dt: 0.02\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Saving network\n",
    "if save:\n",
    "    print(\"Saving network\")\n",
    "    torch.save(policy_net.state_dict(), './policy_net.pt')\n",
    "\n",
    "## Plot\n",
    "def moving_average(a, n=10) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret / n\n",
    "\n",
    "\n",
    "if train:\n",
    "    fig, ax = plt.subplots(4,1)\n",
    "    ax[0].plot(range(1, len(rewards)+1), rewards, label='training reward')\n",
    "    ax[0].plot(moving_average(rewards))\n",
    "    ax[0].set_xlabel('episode')\n",
    "    ax[0].set_ylabel('reward')\n",
    "    ax[0].grid()\n",
    "\n",
    "    ax[1].plot(range(1, len(final_distances)+1), final_distances,'r', label='final distance')\n",
    "    ax[1].scatter(range(1, len(initial_distances)+1), initial_distances, label='initial distance')\n",
    "    ax[1].set_xlabel('episode')\n",
    "    ax[1].set_ylabel('distance')\n",
    "    ax[1].grid()\n",
    "\n",
    "    ax[2].plot(range(1, len(lengths)+1), lengths, label='episode length')\n",
    "    ax[2].set_xlabel('episode')\n",
    "    ax[2].set_ylabel('length')\n",
    "    ax[2].grid()\n",
    "\n",
    "    ax[3].plot(range(1,len(Q_average)+1), Q_average, label='Average Q')\n",
    "    ax[3].set_xlabel('Validation episode')\n",
    "    ax[3].set_ylabel('Average Q')\n",
    "    ax[3].grid()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "env.env.close()\n",
    "\n",
    "\n",
    "### VIEWER\n",
    "\n",
    "# Import environment\n",
    "env = gym.make('CartPoleCrane-v3')\n",
    "\n",
    "env.seed(1234)\n",
    "outdir = 'tmp/dqn-agent-results'\n",
    "env = wrappers.Monitor(env, directory=outdir, force=True)\n",
    "\n",
    "s = env.reset()\n",
    "#s = s_fix_1\n",
    "\n",
    "distace = s[0] - s[2]\n",
    "for _ in range(400):\n",
    "    # Get the q value for all possible moves\n",
    "    q_value_all_actions = policy_net(torch.from_numpy(s).float())\n",
    "    # Select the action that has the highest value\n",
    "    action = q_value_all_actions.argmax().item()\n",
    "\n",
    "    # Perform action and get results\n",
    "    numpy_action = D2C(action) # Discrete action as a numpy variable\n",
    "    next_state, reward, done, _ = env.step(numpy_action)\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "    s = next_state\n",
    "\n",
    "# Close the env and write monitor result info to disk\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained agent\n",
    "The following pretrained agent performs better but is far from perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/casper/Dropbox/DL/gym/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateSpaceDiscrete(\n",
      "array([[-1.89855434e-03, -5.31327561e-03,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 1.20559430e-01,  3.37396440e-01,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [ 6.16210292e-06,  1.74810578e-05,  1.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00],\n",
      "       [-2.18335220e-04,  1.20991136e-03,  0.00000000e+00,\n",
      "         9.94166023e-01, -2.50653640e-01],\n",
      "       [-7.87649478e-06,  1.41947131e-05,  0.00000000e+00,\n",
      "         1.99499836e-02,  9.97491020e-01]]),\n",
      "array([[ 4.41690012e-01],\n",
      "       [ 1.96066911e+01],\n",
      "       [ 3.26992751e-04],\n",
      "       [-3.58022490e-02],\n",
      "       [-4.18525342e-04]]),\n",
      "array([[ 0.        ,  0.        ,  1.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        , 57.29577951]]),\n",
      "array([[0],\n",
      "       [0]]),\n",
      "dt: 0.02\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "policy_net.load_state_dict(torch.load('pendulumCrane/pretrainedNets/dqn_net.pt'))\n",
    "\n",
    "### VIEWER\n",
    "\n",
    "# Import environment\n",
    "env = gym.make('CartPoleCrane-v3')\n",
    "\n",
    "env.seed(1234)\n",
    "outdir = 'tmp/dqn-agent-results'\n",
    "env = wrappers.Monitor(env, directory=outdir, force=True)\n",
    "\n",
    "s = env.reset()\n",
    "#s = s_fix_1\n",
    "\n",
    "distace = s[0] - s[2]\n",
    "for _ in range(400):\n",
    "    # Get the q value for all possible moves\n",
    "    q_value_all_actions = policy_net(torch.from_numpy(s).float())\n",
    "    # Select the action that has the highest value\n",
    "    action = q_value_all_actions.argmax().item()\n",
    "\n",
    "    # Perform action and get results\n",
    "    numpy_action = D2C(action) # Discrete action as a numpy variable\n",
    "    next_state, reward, done, _ = env.step(numpy_action)\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "    s = next_state\n",
    "\n",
    "# Close the env and write monitor result info to disk\n",
    "env.env.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
