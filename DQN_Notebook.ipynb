{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DQN - agent\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import gym\n",
    "\n",
    "from gym import wrappers, logger\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "We use a replay buffer to store transitions and sample from this buffer when training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Replay buffer\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"Experience Replay Memory\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        #self.size = size\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, *args):\n",
    "        \"\"\"Add experience to memory.\"\"\"\n",
    "        self.memory.append([*args])\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample batch of experiences from memory with replacement.\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def count(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Network\n",
    "The network is a standard feed forward neural net, with 3 hidden layers and 128 hidden units in each, with dropout between each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DQN class\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs, learning_rate):\n",
    "        super(DQN, self).__init__()\n",
    "        self.hidden_size = 128\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(n_inputs,self.hidden_size)\n",
    "        self.fc2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.fc3 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        self.out = nn.Linear(self.hidden_size, n_outputs)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "        # define dropout\n",
    "        self.dropout = torch.nn.Dropout(p=0.5, inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Standard FNN layers\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        return F.softmax(x, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions and reward calculation\n",
    "To use a continuous environment, we pass the neuron index of the selected action through a small pseudo DAC, which converts it to a voltage in the D2C function.\n",
    "\n",
    "We also calculate the reward used internally in the agent, as the environment was shared between DQN and DPPG, which uses different reward structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the index of the output neuron, to a continuous value for the environment\n",
    "def D2C(discrete_action):\n",
    "    pos_voltage = 2.0 # Go left\n",
    "    neg_voltage = -2.0 # Go right\n",
    "    volt_range = np.array([-9.0, -7.0, -5.0, -2.0, -1.0, -0.1, 0.0, 0.1, 1.0, 2.0, 5.0, 7.0, 9.0])\n",
    "    output = volt_range[int(discrete_action)]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def agent_reward(action, next_state, reward_array, old_state):\n",
    "\n",
    "    # Calculate distance \n",
    "    x_goal = next_state[2]\n",
    "    x_pos = next_state[0]\n",
    "    distance = x_pos - x_goal\n",
    "\n",
    "    x_goal_old = old_state[2]\n",
    "    x_pos_old = old_state[0]\n",
    "    old_distance = x_pos_old - x_goal_old\n",
    "\n",
    "    # Calculate end effector position of pendulum\n",
    "    theta = next_state[1]\n",
    "\n",
    "    pen_length = 0.7 # Length of pendulum - Placeholder\n",
    "    theta_pos = x_pos + math.sin(theta)*pen_length\n",
    "\n",
    "    # Use distance of end effector instead of sledge\n",
    "    distance_pendulum = theta_pos - x_goal\n",
    "\n",
    "    dist_dif = distance**2 - old_distance**2\n",
    "\n",
    "    reward = 0\n",
    "    reward_type = 0\n",
    "    # Base reward:\n",
    "    if dist_dif > 0:\n",
    "        reward = -1\n",
    "        reward_type = 1\n",
    "    elif dist_dif == 0:\n",
    "        reward = 1\n",
    "        reward_type = 0\n",
    "    elif dist_dif < 0:\n",
    "        reward = 1\n",
    "        reward_type = 2\n",
    "\n",
    "\n",
    "    reward_array[reward_type] += 1 \n",
    "\n",
    "\n",
    "    # Distance bonus\n",
    "    dist_bonus = 2 - abs(distance)*10\n",
    "    reward = reward + dist_bonus\n",
    "\n",
    "    # Theta_pos penalty\n",
    "    theta_p = next_state[1]\n",
    "    theta_p = (x_pos+math.sin(theta_p)*0.7)-x_pos\n",
    "    theta_penalty = abs(theta_p)*2\n",
    "\n",
    "    reward = reward - theta_penalty\n",
    "    return reward, reward_array\n",
    "\n",
    "def random_action(n_outputs):\n",
    "    action = np.random.randint(n_outputs,size=1)\n",
    "    action = float(action)\n",
    "    return action\n",
    "\n",
    "\n",
    "\n",
    "def select_action(state, step_count):\n",
    "    # Determine if we are taking a random action or not\n",
    "    Eps_start = 0.9\n",
    "    Eps_end = 0.15\n",
    "    Eps_decay = 25\n",
    "\n",
    "    sample = random.random()\n",
    "    Eps_threshold = Eps_end + (Eps_start - Eps_end) * math.exp(-1. *step_count/Eps_decay)\n",
    "    #step_count += 1\n",
    "\n",
    "    action_type = \"R\" # Random action\n",
    "\n",
    "    if sample > Eps_threshold: \n",
    "        action_type = \"Q\" # Q action\n",
    "\n",
    "    return action_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "The optimizer samples random transitions from the replay buffer and uses them as a basis for training the network, based on the Huber Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_optimizer(n_inputs):\n",
    "    if replay_memory.count() < batch_size:\n",
    "        return\n",
    "\n",
    "    global num_param_updates\n",
    "    # sample batch from replay memory\n",
    "    batch = np.array(replay_memory.sample(batch_size))#,dtype=float)\n",
    "\n",
    "\n",
    "    # Extract from batch\n",
    "    ss, aa, rr, ss1, dd = np.stack(batch[:,0]), np.stack(batch[:,1]), np.stack(batch[:,2]), np.stack(batch[:,3]), np.stack(batch[:,4]).astype(int)\n",
    "\n",
    "    # Convert to Tensors\n",
    "    ss = torch.from_numpy(ss).float().view(-1,n_inputs)\n",
    "    aa = torch.from_numpy(aa).long().view(-1,1)\n",
    "    rr = torch.from_numpy(rr).float().view(-1,1)\n",
    "    ss1 = torch.from_numpy(ss1).float().view(-1,n_inputs)\n",
    "    dd = torch.from_numpy(dd).float().view(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "    # Forward pass on batch\n",
    "    policy_net.optimizer.zero_grad()\n",
    "    Q = policy_net(ss.float())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get the q value for all possible moves\n",
    "        Q1 = policy_net(ss1.float())\n",
    "\n",
    "    yk = Q1.clone()\n",
    "    for k in range(batch_size):\n",
    "        yk_k = rr[k] + gamma * Q1[k].max().item() * (not dd[k])\n",
    "        yk[k, aa[k]] = yk_k\n",
    "\n",
    "    ## update network weights\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(Q, yk)\n",
    "\n",
    "    # Old loss function\n",
    "    #loss = policy_net.loss(Q, yk)\n",
    "\n",
    "    loss.backward()\n",
    "    policy_net.optimizer.step()\n",
    "\n",
    "    num_param_updates += 1\n",
    "\n",
    "    if num_param_updates % target_update_freq == 0:\n",
    "        # update target network parameters from policy network parameters\n",
    "        target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary training loop\n",
    "Specify the OpenAI Gym environment, and define hyper parameters.\n",
    "Select if the system should load a previously saved network, if the new network should be saved after training, and if the system should train or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] [env_id]\n",
      "__main__.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    " ####### MAIN #######\n",
    "\n",
    "# Import environment\n",
    "parser = argparse.ArgumentParser(description=None)\n",
    "parser.add_argument('env_id', nargs='?', default='CartPoleCraneTrain-v3', help='Select the environment to run')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# You can set the level to logger.DEBUG or logger.WARN if you\n",
    "# want to change the amount of output.\n",
    "logger.set_level(logger.INFO)\n",
    "\n",
    "env = gym.make(args.env_id)\n",
    "\n",
    "\n",
    "# Number of inputs and outputs\t\n",
    "n_inputs = 3 \n",
    "n_outputs = 13 \n",
    "print(\"Number of inputs: \", n_inputs, \", number of outputs: \", n_outputs)\n",
    "\n",
    "\n",
    "\n",
    "##### train Deep Q-network #####\n",
    "\n",
    "# Parameters\n",
    "num_episodes = 20 # default: 50 # Number of episodes\n",
    "episode_limit = 800 # Length of episode\n",
    "\n",
    "batch_size = 32 # Default: 32\n",
    "learning_rate = 0.002 # Default = 0.002\n",
    "gamma = 0.99 # discount rate\n",
    "replay_memory_capacity = 5000\n",
    "prefill_memory = False\n",
    "val_freq = 10 # validation frequency\n",
    "\n",
    "\n",
    "num_param_updates = 0\n",
    "target_update_freq = 25 # Default 25\n",
    "\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "dlongtype = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor\n",
    "\n",
    "# initialize DQN\n",
    "policy_net = DQN(n_inputs, n_outputs, learning_rate).to(device)\n",
    "target_net = DQN(n_inputs, n_outputs, learning_rate).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "\n",
    "## Load network \n",
    "load = 1\n",
    "save = 0\n",
    "train = 0\n",
    "if load:\n",
    "\tprint(\"Loading network\")\n",
    "\tpolicy_net.load_state_dict(torch.load('./policy_net.pt'))\n",
    "\ttarget_net.load_state_dict(torch.load('./policy_net.pt'))\n",
    "\n",
    "if not train:\n",
    "\tprint(\"Skipping Training\")\n",
    "\tdata_array = np.array([0,0,0,0,0,0])\n",
    "\tnum_episodes = 0\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = ReplayMemory(replay_memory_capacity)\n",
    "\n",
    "#eps_check(num_episodes)\n",
    "\n",
    "val_ep = False\n",
    "\n",
    "\n",
    "# Tau from environment\n",
    "tau_agent = env.env.tau\n",
    "print(\"Tau: \", tau_agent, \" Number of steps: \", episode_limit, \" Episode duration: \", tau_agent*episode_limit, \" [s]\")\n",
    "\n",
    "# Collect a set of fixed states for average Q metric\n",
    "state = env.reset()\n",
    "\n",
    "s_fix_1 = state # x, theta, goal\n",
    "s_fix_1[0] = 0.05\n",
    "s_fix_1[2] = 0.95\n",
    "\n",
    "state = env.reset()\n",
    "s_fix_2 = state\n",
    "s_fix_2[0] = 0.95\n",
    "s_fix_2[2] = 0.05\n",
    "\n",
    "state = env.reset()\n",
    "s_fix_3 = state\n",
    "s_fix_3[0] = 0.25\n",
    "s_fix_3[2] = 0.75\n",
    "\n",
    "state = env.reset()\n",
    "s_fix_4 = state\n",
    "s_fix_4[0] = 0.75\n",
    "s_fix_4[2] = 0.25\n",
    "\n",
    "fixed_states = np.array([s_fix_1, s_fix_2, s_fix_3, s_fix_4])\n",
    "print(\"Fixed states: \", fixed_states)\n",
    "\n",
    "Q_average = []\n",
    "\n",
    "output_histogram = np.zeros(n_outputs)\n",
    "reward_array = np.zeros(4)\n",
    "\n",
    "\n",
    "## Training loop\n",
    "print(\"Start training\")\n",
    "rewards, lengths = [], []\n",
    "final_distances = []\n",
    "initial_distances = []\n",
    "state = env.reset()\n",
    "\n",
    "## Data\n",
    "#data_array = np.array([0,0,0,0])\n",
    "#print(\"Size of array :\", data_array.size)\n",
    "\n",
    "\n",
    "epsilon = 1.0\n",
    "\n",
    "step_count = 0\n",
    "max_theta = 0\n",
    "for i in range(num_episodes):\n",
    "\n",
    "\t# Reset environment and values\n",
    "\t# Get initial observation \n",
    "\tstate = env.reset()\n",
    "\tep_reward = 0\n",
    "\tfinal_distance = 0\n",
    "\n",
    "\t# Initialize Action array storage\n",
    "\taction_array = np.array([[0,0,0,0]])\n",
    "\n",
    "\t# Initial distance\n",
    "\tx_goal = state[2]\n",
    "\tx_pos = state[0]\n",
    "\tinitial_distance = x_pos - x_goal\n",
    "\tdistance = initial_distance\n",
    "\n",
    "\tfor j in range(episode_limit):\n",
    "\n",
    "\t\t# Select action based on states, the epsilon greedy strategy and validation episode flag\n",
    "\t\taction_type = select_action(state, step_count)\n",
    "\n",
    "\t\t#if np.random.rand() >= epsilon:\n",
    "\t\tif action_type == \"Q\":\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\t# Get the q value for all possible moves\n",
    "\t\t\t\tq_value_all_actions = policy_net(torch.from_numpy(state).float())\n",
    "\t\t\t\t# Select the action that has the highest value\n",
    "\t\t\t\taction = q_value_all_actions.argmax().item()\n",
    "\n",
    "\t\t\t\t# Update output histogram\n",
    "\t\t\t\toutput_histogram[int(action)] += 1\n",
    "\n",
    "\t\telse:\n",
    "\t\t\taction = random_action(n_outputs)\n",
    "\n",
    "\t\t# Perform action and get results\n",
    "\t\tnumpy_action = D2C(action) # Discrete action as a numpy variable\n",
    "\t\tnext_state, reward_old, done, _ = env.step(numpy_action)\n",
    "\n",
    "\t\t# Old distance\n",
    "\t\told_distance = distance\n",
    "\n",
    "\t\t# New distance \n",
    "\t\tx_goal = next_state[2]\n",
    "\t\tx_pos = next_state[0]\n",
    "\t\tdistance = x_pos - x_goal\n",
    "\n",
    "\n",
    "\t\t# get reward\n",
    "\t\treward, reward_array = agent_reward(numpy_action, next_state, reward_array, state)\n",
    "\n",
    "\t\t# Check if done\n",
    "\t\tif done:\n",
    "\t\t\tlengths.append(j + 1)\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\treplay_memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "\t\t# Perform model optimization (It automatically checks if replay buffer is full)\n",
    "\t\tif (j+1) % 2 == 0:\n",
    "\t\t\tsimple_optimizer(n_inputs)\n",
    "\n",
    "\n",
    "\t\t# bookkeeping\n",
    "\t\tstate = next_state\n",
    "\t\tep_reward += reward\n",
    "\n",
    "\t### End of episode\n",
    "\t\n",
    "\tfinal_distances.append(distance)\n",
    "\trewards.append(ep_reward)\n",
    "\tlengths.append(j+1)\n",
    "\tinitial_distances.append(initial_distance)\n",
    "\n",
    "\n",
    "\n",
    "\tstep_count += 1\n",
    "\t# Validation episode\n",
    "\tif (i+1) % val_freq == 0:\n",
    "\t\tvalidation_rewards = []\n",
    "\t\t#q_diff = []\n",
    "\t\tfor ii in range(4):\n",
    "\t\t\tdata_array = np.array([0,0,0,0,0,0])\n",
    "\t\t\ts = env.reset()\n",
    "\n",
    "\t\t\t# Fixed state for validation\n",
    "\t\t\ts[0] = 0.05 # Start pos\n",
    "\t\t\ts[2] = 0.80 # Goal pos\n",
    "\t\t\tenv.env.state[2] = s[0]\n",
    "\t\t\tenv.env.set_goal(s[2])\n",
    "\n",
    "\t\t\treward_val = 0\n",
    "\t\t\tfor jj in range(episode_limit):\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\t# Get the q value for all possible moves\n",
    "\t\t\t\t\tQ_probs = policy_net(torch.from_numpy(s).float())\n",
    "\t\t\t\t\t# Select the action that has the highest value\n",
    "\t\t\t\t\taction = Q_probs.argmax().item()\n",
    "\n",
    "\t\t\t\ta_env = D2C(action)\n",
    "\t\t\t\ts_new, r_old, done, _ = env.step(a_env)\n",
    "\n",
    "\t\t\t\told_distance = distance\n",
    "\t\t\t\t# Update distance\n",
    "\t\t\t\tx_goal = s[2]\n",
    "\t\t\t\tx_pos = s[0]\n",
    "\t\t\t\tdistance = x_pos - x_goal\n",
    "\n",
    "\t\t\t\t# Calculate actual reward\n",
    "\t\t\t\tr, reward_array = agent_reward(a_env, s_new, reward_array, s)\n",
    "\t\t\t\t#q_diff = qd\n",
    "\n",
    "\t\t\t\ttime_stamp = tau_agent*jj\n",
    "\t\t\t\t## Data \n",
    "\t\t\t\treward_val += r\n",
    "\t\t\t\ts = s_new\n",
    "\t\t\t\tif done: \n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t#print(\"Breaking, ii: \", ii, \" jj: \", jj)\n",
    "\t\t\tvalidation_rewards.append(reward_val)\n",
    "\t\tprint('{:4d}. mean training reward: {:6.2f}, mean validation reward: {:6.2f}'.format(i+1, np.mean(rewards[-val_freq:]), np.mean(validation_rewards)))\n",
    "\n",
    "\t\t# Average Q value on fixed states\n",
    "\t\tQ_ep = []\n",
    "\t\tfor ii in range(4):\n",
    "\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\t# Get the q value for all possible moves\n",
    "\t\t\t\tQ_probs = policy_net(torch.from_numpy(fixed_states[ii]).float())\n",
    "\t\t\t# get the highest value action\n",
    "\t\t\tmax_action = Q_probs.max().item()\n",
    "\t\t\tQ_ep.append(max_action)\n",
    "\t\tQ_average.append(np.mean(Q_ep))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After training\n",
    "Functions to save the network, plot general data and display a simulation of the resulting network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Saving network\n",
    "if save:\n",
    "    print(\"Saving network\")\n",
    "    torch.save(policy_net.state_dict(), './policy_net.pt')\n",
    "\n",
    "## Plot\n",
    "def moving_average(a, n=10) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret / n\n",
    "\n",
    "\n",
    "if train:\n",
    "    fig, ax = plt.subplots(4,1)\n",
    "    ax[0].plot(range(1, len(rewards)+1), rewards, label='training reward')\n",
    "    ax[0].plot(moving_average(rewards))\n",
    "    ax[0].set_xlabel('episode')\n",
    "    ax[0].set_ylabel('reward')\n",
    "    ax[0].grid()\n",
    "\n",
    "    ax[1].plot(range(1, len(final_distances)+1), final_distances,'r', label='final distance')\n",
    "    ax[1].scatter(range(1, len(initial_distances)+1), initial_distances, label='initial distance')\n",
    "    ax[1].set_xlabel('episode')\n",
    "    ax[1].set_ylabel('distance')\n",
    "    ax[1].grid()\n",
    "\n",
    "    ax[2].plot(range(1, len(lengths)+1), lengths, label='episode length')\n",
    "    ax[2].set_xlabel('episode')\n",
    "    ax[2].set_ylabel('length')\n",
    "    ax[2].grid()\n",
    "\n",
    "    ax[3].plot(range(1,len(Q_average)+1), Q_average, label='Average Q')\n",
    "    ax[3].set_xlabel('Validation episode')\n",
    "    ax[3].set_ylabel('Average Q')\n",
    "    ax[3].grid()\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    objects_1 = ('0', '-1', '1', 'Golden 0')\n",
    "    y_pos_1 = np.arange(len(objects_1))\n",
    "    plt.bar(y_pos_1, reward_array, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos_1, objects_1)\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "env.env.close()\n",
    "\n",
    "\n",
    "### VIEWER\n",
    "\n",
    "# Import environment\n",
    "parser = argparse.ArgumentParser(description=None)\n",
    "parser.add_argument('env_id', nargs='?', default='CartPoleCraneTrain-v3', help='Select the environment to run')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# You can set the level to logger.DEBUG or logger.WARN if you\n",
    "# want to change the amount of output.\n",
    "logger.set_level(logger.INFO)\n",
    "\n",
    "env = gym.make(args.env_id)\n",
    "\n",
    "env.seed(12345)\n",
    "outdir = '/tmp/random-agent-results'\n",
    "env = wrappers.Monitor(env, directory=outdir, force=True)\n",
    "\n",
    "s = env.reset()\n",
    "#s = s_fix_1\n",
    "\n",
    "distace = s[0] - s[2]\n",
    "for _ in range(400):\n",
    "    # Get the q value for all possible moves\n",
    "    q_value_all_actions = policy_net(torch.from_numpy(s).float())\n",
    "    # Select the action that has the highest value\n",
    "    action = q_value_all_actions.argmax().item()\n",
    "\n",
    "    # Perform action and get results\n",
    "    numpy_action = D2C(action) # Discrete action as a numpy variable\n",
    "    next_state, reward, done, _ = env.step(numpy_action)\n",
    "\n",
    "    s = next_state\n",
    "\n",
    "# Close the env and write monitor result info to disk\n",
    "env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
